{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbae7d4e",
   "metadata": {},
   "source": [
    "# Scaling up Web Scraping with Ray\n",
    "\n",
    "In this example we will show you how to use Ray for scraping information from the web. There are sophisticated Python libraries to achieve this task (like [https://scrapy.org/](https://scrapy.org/)). In this example we will keep it very simple and adapt existing code from [https://www.scrapingbee.com/blog/crawling-python/](https://www.scrapingbee.com/blog/crawling-python/) and show how easy it is to parallelize the code with Ray. Ray is well suited for scaling up web scraping: While for batch-processing systems we need a database or other stateful component to hold the list of crawled URLs, Ray's actors allow us to do everything in one framework.\n",
    "\n",
    "First install the required dependencies with\n",
    "\n",
    "```\n",
    "pip install requests bs4\n",
    "```\n",
    "\n",
    "We can then already run the example from [https://www.scrapingbee.com/blog/crawling-python/](https://www.scrapingbee.com/blog/crawling-python/) out of the box like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeadd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11475df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "\n",
    "    def __init__(self, urls=[]):\n",
    "        self.visited_urls = []\n",
    "        self.urls_to_visit = urls\n",
    "\n",
    "    def download_url(self, url):\n",
    "        text = requests.get(url).text\n",
    "        return text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def add_url_to_visit(self, url):\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            self.urls_to_visit.append(url)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        for url in self.get_linked_urls(url, html):\n",
    "            self.add_url_to_visit(url)\n",
    "\n",
    "    def run(self):\n",
    "        while self.urls_to_visit:\n",
    "            url = self.urls_to_visit.pop(0)\n",
    "            logging.info(f'URLs: {len(self.visited_urls) + len(self.urls_to_visit)}')\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "            except Exception:\n",
    "                pass\n",
    "                # logging.exception(f'Failed to crawl: {url}')\n",
    "            finally:\n",
    "                self.visited_urls.append(url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Crawler(urls=['https://en.wikipedia.org/']).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfb6cf4",
   "metadata": {},
   "source": [
    "In order to parallelize the crawling, let us first initialize Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize Ray's profiling feature to get insights into where the bottlenecks of the application are.\n",
    "import os\n",
    "os.environ[\"RAY_PROFILING\"] = \"1\"\n",
    "\n",
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32672a",
   "metadata": {},
   "source": [
    "We need to keep track of which URLs we already crawled to avoid double visiting them and we also need to keep track of which URLs still need to be visited. We do this by centralize this data in an actor `CrawlQueue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86477507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import asyncio\n",
    "import collections\n",
    "\n",
    "@ray.remote\n",
    "class CrawlQueue:\n",
    "    # Initialize the crawl queue with a set of seed urls to be crawled.\n",
    "    async def __init__(self, seed_urls):\n",
    "        logging.basicConfig(\n",
    "            format='%(asctime)s %(levelname)s:%(message)s',\n",
    "            level=logging.INFO)\n",
    "        # A queue of pending crawl requests\n",
    "        self.pending_crawl_requests = asyncio.Queue()\n",
    "        # All crawl requests - pending, in progress, and completed\n",
    "        self.all_crawl_requests = set()\n",
    "        for url in seed_urls:\n",
    "            await self.add_crawl_request(url)\n",
    "\n",
    "    # Add additional urls to be crawled - this is called each time a crawler\n",
    "    # encounters a URL in the document it is processing.\n",
    "    async def add_crawl_request(self, url):\n",
    "        if url not in self.all_crawl_requests:\n",
    "            await self.pending_crawl_requests.put(url)\n",
    "            self.all_crawl_requests.add(url)\n",
    "\n",
    "    # Get an url to crawl - this is called from an idle crawler.\n",
    "    # It returns the url to be crawled.\n",
    "    async def get_crawl_request(self):\n",
    "        logging.info(f'URLs: {len(self.all_crawl_requests)}')\n",
    "        return await self.pending_crawl_requests.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934ab6c",
   "metadata": {},
   "source": [
    "<!-- #raw -->\n",
    "```{eval-rst}\n",
    ".. code-block:: python\n",
    "    :emphasize-lines: 21, 30, 39, 40, 42, 45, 46\n",
    "\n",
    "    class RayCrawler:\n",
    "\n",
    "        def __init__(self, crawl_queue):\n",
    "            self.crawl_queue = crawl_queue\n",
    "            self.num_processed_bytes = 0\n",
    "\n",
    "        def download_url(self, url):\n",
    "            text = requests.get(url).text\n",
    "            self.num_processed_bytes += len(text)\n",
    "            return text\n",
    "\n",
    "        def get_linked_urls(self, url, html):\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                path = link.get('href')\n",
    "                if path and path.startswith('/'):\n",
    "                    path = urljoin(url, path)\n",
    "                yield path\n",
    "\n",
    "        def add_url_to_visit(self, url):\n",
    "            self.crawl_queue.add_crawl_request.remote(url)\n",
    "\n",
    "        def crawl(self, url):\n",
    "            html = self.download_url(url)\n",
    "            for url in self.get_linked_urls(url, html):\n",
    "                self.add_url_to_visit(url)\n",
    "\n",
    "        def run(self):\n",
    "            while True:\n",
    "                url = ray.get(self.crawl_queue.get_crawl_request.remote())\n",
    "                logging.info(f'Crawling: {url}')\n",
    "                logging.info(f'Bytes: {self.num_processed_bytes}')\n",
    "                try:\n",
    "                    self.crawl(url)\n",
    "                except Exception:\n",
    "                    # logging.exception(f'Failed to crawl: {url}')\n",
    "                    pass\n",
    "                \n",
    "    @ray.remote\n",
    "    def worker(crawl_queue):\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        RayCrawler(crawl_queue).run()\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        crawl_queue = CrawlQueue.remote(['https://en.wikipedia.org/'])\n",
    "        ray.get([worker.remote(crawl_queue) for i in range(5)])\n",
    "```\n",
    "```\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:42,862 INFO:URLs: 1\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:42,863 INFO:URLs: 1\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:42,863 INFO:URLs: 1\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:42,863 INFO:URLs: 1\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:43,065 INFO:URLs: 40\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:43,065 INFO:URLs: 40\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:43,065 INFO:URLs: 40\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:43,161 INFO:URLs: 262\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:43,264 INFO:URLs: 327\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:43,637 INFO:URLs: 997\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:43,904 INFO:URLs: 1664\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:44,828 INFO:URLs: 3537\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:44,933 INFO:URLs: 3740\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:45,448 INFO:URLs: 5085\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:46,652 INFO:URLs: 7862\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:46,723 INFO:URLs: 7979\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:46,744 INFO:URLs: 7979\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:47,662 INFO:URLs: 10000\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:47,711 INFO:URLs: 10079\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:48,267 INFO:URLs: 11288\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:49,029 INFO:URLs: 12996\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:49,069 INFO:URLs: 13054\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:49,903 INFO:URLs: 14534\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:50,250 INFO:URLs: 15093\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:50,622 INFO:URLs: 15850\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:50,678 INFO:URLs: 15961\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:50,890 INFO:URLs: 16365\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:51,030 INFO:URLs: 16596\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:51,331 INFO:URLs: 17192\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:51,335 INFO:URLs: 17198\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:51,434 INFO:URLs: 17350\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:51,746 INFO:URLs: 17486\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:51,927 INFO:URLs: 17806\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:51,980 INFO:URLs: 17875\n",
    "2022-06-24 01:23:52,564\tWARNING worker.py:1404 -- Warning: More than 5000 tasks are pending submission to actor 5f0fdb5035e172d9bb00d39b01000000. To reduce memory usage, wait for these tasks to finish before sending more.\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:53,238 INFO:URLs: 20569\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:54,835 INFO:URLs: 25175\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:55,689 INFO:URLs: 26614\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:56,443 INFO:URLs: 27648\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:57,483 INFO:URLs: 29637\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:58,491 INFO:URLs: 31531\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:59,273 INFO:URLs: 32863\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:23:59,978 INFO:URLs: 33858\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:00,229 INFO:URLs: 34293\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:00,699 INFO:URLs: 35119\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:00,791 INFO:URLs: 35281\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:00,987 INFO:URLs: 35740\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:01,034 INFO:URLs: 35796\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:01,102 INFO:URLs: 35916\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:01,404 INFO:URLs: 36113\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:01,848 INFO:URLs: 36783\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:01,922 INFO:URLs: 36918\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:02,332 INFO:URLs: 37714\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:02,799 INFO:URLs: 38609\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:03,169 INFO:URLs: 39630\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:03,547 INFO:URLs: 40015\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:03,630 INFO:URLs: 40130\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:04,569 INFO:URLs: 41238\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:05,444 INFO:URLs: 42255\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:05,493 INFO:URLs: 42299\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:06,600 INFO:URLs: 43529\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:06,712 INFO:URLs: 43615\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:06,984 INFO:URLs: 43969\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:08,501 INFO:URLs: 46122\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:08,605 INFO:URLs: 46244\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:10,775 INFO:URLs: 49757\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:10,867 INFO:URLs: 49930\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:11,866 INFO:URLs: 50991\n",
    "2022-06-24 01:24:12,448\tWARNING worker.py:1404 -- Warning: More than 5000 tasks are pending submission to actor 5f0fdb5035e172d9bb00d39b01000000. To reduce memory usage, wait for these tasks to finish before sending more.\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:12,695 INFO:URLs: 52009\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:13,434 INFO:URLs: 53378\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:15,572 INFO:URLs: 56747\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:18,551 INFO:URLs: 61107\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:19,181 INFO:URLs: 62028\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:19,717 INFO:URLs: 62908\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:19,879 INFO:URLs: 63109\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:20,053 INFO:URLs: 63311\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:20,148 INFO:URLs: 63417\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:20,415 INFO:URLs: 63798\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:20,535 INFO:URLs: 63931\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:21,370 INFO:URLs: 65354\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:21,406 INFO:URLs: 65423\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:21,560 INFO:URLs: 65674\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:22,653 INFO:URLs: 66384\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:23,023 INFO:URLs: 66932\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:23,153 INFO:URLs: 67007\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:24,631 INFO:URLs: 69091\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:24,798 INFO:URLs: 69352\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:27,194 INFO:URLs: 72397\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:27,276 INFO:URLs: 72568\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:28,088 INFO:URLs: 73582\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:28,089 INFO:URLs: 73582\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:28,413 INFO:URLs: 74055\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:29,438 INFO:URLs: 75363\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:29,832 INFO:URLs: 75932\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:30,353 INFO:URLs: 76676\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:31,444 INFO:URLs: 77963\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:31,774 INFO:URLs: 78581\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:32,001 INFO:URLs: 78909\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:32,288 INFO:URLs: 79222\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:32,746 INFO:URLs: 79767\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:33,328 INFO:URLs: 80571\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:34,868 INFO:URLs: 82719\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:35,182 INFO:URLs: 83124\n",
    "\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:36,195 INFO:URLs: 84420\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:36,733 INFO:URLs: 85094\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:36,935 INFO:URLs: 85221\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:37,995 INFO:URLs: 86190\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:38,816 INFO:URLs: 86995\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:39,070 INFO:URLs: 87348\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:39,215 INFO:URLs: 87517\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:39,383 INFO:URLs: 87707\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:40,026 INFO:URLs: 88526\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:40,654 INFO:URLs: 89269\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:40,856 INFO:URLs: 89389\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:41,212 INFO:URLs: 89914\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:41,452 INFO:URLs: 90209\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:41,878 INFO:URLs: 90791\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,152 INFO:URLs: 91179\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,213 INFO:URLs: 91263\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,285 INFO:URLs: 91341\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,352 INFO:URLs: 91407\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,419 INFO:URLs: 91462\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,507 INFO:URLs: 91567\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,569 INFO:URLs: 91629\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,654 INFO:URLs: 91672\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,795 INFO:URLs: 91802\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,882 INFO:URLs: 91849\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:42,948 INFO:URLs: 91900\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:43,220 INFO:URLs: 92254\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:43,302 INFO:URLs: 92331\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:43,441 INFO:URLs: 92521\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:43,662 INFO:URLs: 92930\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:43,793 INFO:URLs: 93069\n",
    "(CrawlQueue pid=20236) 2022-06-24 01:24:44,032 INFO:URLs: 93459\n",
    "```\n",
    "<!-- #endraw -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e8473",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class RayCrawler:\n",
    "\n",
    "    def __init__(self, crawl_queue):\n",
    "        self.crawl_queue = crawl_queue\n",
    "\n",
    "    def download_url(self, url):\n",
    "        text = requests.get(url).text\n",
    "        return text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def add_url_to_visit(self, url):\n",
    "        self.crawl_queue.add_crawl_request.remote(url)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        for url in self.get_linked_urls(url, html):\n",
    "            self.add_url_to_visit(url)\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            url = ray.get(self.crawl_queue.get_crawl_request.remote())\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "            except Exception:\n",
    "                # logging.exception(f'Failed to crawl: {url}')\n",
    "                pass\n",
    "                \n",
    "@ray.remote\n",
    "def worker(crawl_queue):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    RayCrawler(crawl_queue).run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawl_queue = CrawlQueue.remote(['https://en.wikipedia.org/'])\n",
    "    ray.get([worker.remote(crawl_queue) for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5a8c0",
   "metadata": {},
   "source": [
    "### Profiling the application\n",
    "\n",
    "In order to profile where the bottlenecks of the application are, we save the task timeline to `/tmp/timeline.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a00145",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.timeline(filename=\"/tmp/timeline.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3e674f4",
   "metadata": {},
   "source": [
    "We can load the timeline in Chrome Tracing -- open the Chrome browser, navigate to `chrome://tracing`, click the `Load` button and select the `/tmp/timeline.json` file. After zooming into the timeline a bit you should see a timeline view like\n",
    "\n",
    "```{image} ../../images/web_crawler_timeline.png\n",
    ":align: center\n",
    "```\n",
    "\n",
    "If you select one of the grey tasks, you see that it is from `CrawlQueue.add_crawl_request()` and there are lots of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86b64a",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import asyncio\n",
    "import collections\n",
    "\n",
    "@ray.remote\n",
    "class BatchedCrawlQueue:\n",
    "    # Initialize the crawl queue with a set of seed urls to be crawled.\n",
    "    async def __init__(self, seed_urls):\n",
    "        logging.basicConfig(\n",
    "            format='%(asctime)s %(levelname)s:%(message)s',\n",
    "            level=logging.INFO)\n",
    "        # A queue of pending crawl requests\n",
    "        self.pending_crawl_requests = asyncio.Queue()\n",
    "        # All crawl requests - pending, in progress, and completed\n",
    "        self.all_crawl_requests = set()\n",
    "        await self.add_crawl_requests(seed_urls)\n",
    "\n",
    "    # Add additional urls to be crawled - this is called each time a crawler\n",
    "    # encounters a URL in the document it is processing.\n",
    "    async def add_crawl_requests(self, urls):\n",
    "        for url in urls:\n",
    "            if url not in self.all_crawl_requests:\n",
    "                await self.pending_crawl_requests.put(url)\n",
    "                self.all_crawl_requests.add(url)\n",
    "\n",
    "    # Get an url to crawl - this is called from an idle crawler.\n",
    "    # It returns the url to be crawled.\n",
    "    async def get_crawl_request(self):\n",
    "        logging.info(f'URLs: {len(self.all_crawl_requests)}')\n",
    "        return await self.pending_crawl_requests.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0316186",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class BatchedRayCrawler:\n",
    "\n",
    "    def __init__(self, crawl_queue):\n",
    "        self.crawl_queue = crawl_queue\n",
    "\n",
    "    def download_url(self, url):\n",
    "        text = requests.get(url).text\n",
    "        return text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        urls = []\n",
    "        for x in self.get_linked_urls(url, html):\n",
    "            urls.append(x)\n",
    "        self.crawl_queue.add_crawl_requests.remote(urls)\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            url = ray.get(self.crawl_queue.get_crawl_request.remote())\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "                logging.info(url)\n",
    "            except Exception:\n",
    "                # logging.exception(f'Failed to crawl: {url}')\n",
    "                pass\n",
    "                \n",
    "@ray.remote\n",
    "def worker(crawl_queue):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    BatchedRayCrawler(crawl_queue).run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawl_queue = BatchedCrawlQueue.remote(['https://en.wikipedia.org/'])\n",
    "    ray.get([worker.remote(crawl_queue) for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef10af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
