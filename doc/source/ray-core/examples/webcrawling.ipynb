{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e5e3cf",
   "metadata": {},
   "source": [
    "# Using Ray for Web Scraping\n",
    "\n",
    "In this example we will show you how to use Ray for scraping information from the web. There are sophisticated Python libraries to achieve this task (like [https://scrapy.org/](https://scrapy.org/)). In this example we will keep it very simple and adapt existing code from [https://www.scrapingbee.com/blog/crawling-python/](https://www.scrapingbee.com/blog/crawling-python/) and show how simple it is to parallelize the code with Ray.\n",
    "\n",
    "First install the required dependencies with\n",
    "\n",
    "```\n",
    "pip install requests bs4\n",
    "```\n",
    "\n",
    "We can then already run the example from [https://www.scrapingbee.com/blog/crawling-python/](https://www.scrapingbee.com/blog/crawling-python/) out of the box like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b594e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c52bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 01:22:09,896 INFO:URLs: 0\n",
      "2022-06-24 01:22:10,168 INFO:URLs: 264\n",
      "2022-06-24 01:22:10,169 INFO:URLs: 264\n",
      "2022-06-24 01:22:10,170 INFO:URLs: 264\n",
      "2022-06-24 01:22:10,171 INFO:URLs: 264\n",
      "2022-06-24 01:22:11,137 INFO:URLs: 3310\n",
      "2022-06-24 01:22:11,535 INFO:URLs: 3899\n",
      "2022-06-24 01:22:12,667 INFO:URLs: 4520\n",
      "2022-06-24 01:22:12,940 INFO:URLs: 4579\n",
      "2022-06-24 01:22:13,424 INFO:URLs: 4651\n",
      "2022-06-24 01:22:14,379 INFO:URLs: 7433\n",
      "2022-06-24 01:22:14,571 INFO:URLs: 7868\n",
      "2022-06-24 01:22:14,727 INFO:URLs: 7986\n",
      "2022-06-24 01:22:14,820 INFO:URLs: 8045\n",
      "2022-06-24 01:22:15,710 INFO:URLs: 10009\n",
      "2022-06-24 01:22:15,809 INFO:URLs: 10089\n",
      "2022-06-24 01:22:16,260 INFO:URLs: 11088\n",
      "2022-06-24 01:22:16,413 INFO:URLs: 11147\n",
      "2022-06-24 01:22:17,254 INFO:URLs: 13067\n",
      "2022-06-24 01:22:18,097 INFO:URLs: 14548\n",
      "2022-06-24 01:22:18,512 INFO:URLs: 15108\n",
      "2022-06-24 01:22:18,921 INFO:URLs: 15661\n",
      "2022-06-24 01:22:19,220 INFO:URLs: 15978\n",
      "2022-06-24 01:22:19,464 INFO:URLs: 16383\n",
      "2022-06-24 01:22:19,677 INFO:URLs: 16615\n",
      "2022-06-24 01:22:20,002 INFO:URLs: 17212\n",
      "2022-06-24 01:22:20,535 INFO:URLs: 17219\n",
      "2022-06-24 01:22:22,856 INFO:URLs: 23447\n",
      "2022-06-24 01:22:23,071 INFO:URLs: 23599\n",
      "2022-06-24 01:22:23,793 INFO:URLs: 24673\n",
      "2022-06-24 01:22:23,940 INFO:URLs: 24810\n",
      "2022-06-24 01:22:24,208 INFO:URLs: 25131\n",
      "2022-06-24 01:22:24,332 INFO:URLs: 25201\n",
      "2022-06-24 01:22:26,691 INFO:URLs: 28408\n",
      "2022-06-24 01:22:28,899 INFO:URLs: 29531\n",
      "2022-06-24 01:22:29,103 INFO:URLs: 29666\n",
      "2022-06-24 01:22:30,444 INFO:URLs: 31561\n",
      "2022-06-24 01:22:31,508 INFO:URLs: 32557\n",
      "2022-06-24 01:22:31,960 INFO:URLs: 33063\n",
      "2022-06-24 01:22:33,357 INFO:URLs: 34326\n",
      "2022-06-24 01:22:34,167 INFO:URLs: 35153\n",
      "2022-06-24 01:22:34,375 INFO:URLs: 35316\n",
      "2022-06-24 01:22:34,813 INFO:URLs: 35898\n",
      "2022-06-24 01:22:35,007 INFO:URLs: 35954\n",
      "2022-06-24 01:22:35,393 INFO:URLs: 36414\n",
      "2022-06-24 01:22:35,573 INFO:URLs: 36534\n",
      "2022-06-24 01:22:36,187 INFO:URLs: 37204\n",
      "2022-06-24 01:22:36,435 INFO:URLs: 37402\n",
      "2022-06-24 01:22:37,422 INFO:URLs: 38569\n",
      "2022-06-24 01:22:37,572 INFO:URLs: 38652\n",
      "2022-06-24 01:22:38,286 INFO:URLs: 39674\n",
      "2022-06-24 01:22:38,637 INFO:URLs: 40060\n",
      "2022-06-24 01:22:42,470 INFO:URLs: 41834\n",
      "2022-06-24 01:22:42,714 INFO:URLs: 41950\n",
      "2022-06-24 01:22:43,271 INFO:URLs: 42462\n",
      "2022-06-24 01:22:43,789 INFO:URLs: 42853\n",
      "2022-06-24 01:22:44,878 INFO:URLs: 43579\n",
      "2022-06-24 01:22:45,100 INFO:URLs: 43666\n",
      "2022-06-24 01:22:45,542 INFO:URLs: 44021\n",
      "2022-06-24 01:22:45,798 INFO:URLs: 44162\n",
      "2022-06-24 01:22:48,216 INFO:URLs: 46298\n",
      "2022-06-24 01:22:50,843 INFO:URLs: 48551\n",
      "2022-06-24 01:22:52,480 INFO:URLs: 49986\n",
      "2022-06-24 01:22:54,478 INFO:URLs: 51489\n",
      "2022-06-24 01:22:55,610 INFO:URLs: 52067\n",
      "2022-06-24 01:22:57,217 INFO:URLs: 53437\n",
      "2022-06-24 01:23:01,372 INFO:URLs: 56807\n",
      "2022-06-24 01:23:05,781 INFO:URLs: 59941\n",
      "2022-06-24 01:23:08,210 INFO:URLs: 62090\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff97f2b08f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mCrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'https://en.wikipedia.org/'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ff97f2b08f15>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'URLs: {len(self.visited_urls) + len(self.urls_to_visit)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ff97f2b08f15>\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_linked_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_url_to_visit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ff97f2b08f15>\u001b[0m in \u001b[0;36mget_linked_urls\u001b[0;34m(self, url, html)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/parse.py\u001b[0m in \u001b[0;36murljoin\u001b[0;34m(base, url, allow_fragments)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fragments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fragments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscheme\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbscheme\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mscheme\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muses_relative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/parse.py\u001b[0m in \u001b[0;36murlparse\u001b[0;34m(url, scheme, allow_fragments)\u001b[0m\n\u001b[1;32m    366\u001b[0m     (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n\u001b[1;32m    367\u001b[0m     \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_coerce_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_coerce_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0msplitresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fragments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muses_params\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m';'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/parse.py\u001b[0m in \u001b[0;36murlsplit\u001b[0;34m(url, scheme, allow_fragments)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_coerce_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_parse_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mMAX_CACHE_SIZE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# avoid runaway growth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0mclear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0mnetloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Crawler:\n",
    "\n",
    "    def __init__(self, urls=[]):\n",
    "        self.visited_urls = []\n",
    "        self.urls_to_visit = urls\n",
    "\n",
    "    def download_url(self, url):\n",
    "        text = requests.get(url).text\n",
    "        return text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def add_url_to_visit(self, url):\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            self.urls_to_visit.append(url)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        for url in self.get_linked_urls(url, html):\n",
    "            self.add_url_to_visit(url)\n",
    "\n",
    "    def run(self):\n",
    "        while self.urls_to_visit:\n",
    "            url = self.urls_to_visit.pop(0)\n",
    "            logging.info(f'URLs: {len(self.visited_urls) + len(self.urls_to_visit)}')\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "            except Exception:\n",
    "                pass\n",
    "                # logging.exception(f'Failed to crawl: {url}')\n",
    "            finally:\n",
    "                self.visited_urls.append(url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Crawler(urls=['https://en.wikipedia.org/']).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3379269f",
   "metadata": {},
   "source": [
    "In order to parallelize the crawling, let us first initialize Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f4dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 01:23:28,882\tINFO services.py:1476 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8267', python_version='3.7.4', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-06-24_01-23-26_513063_20104/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-24_01-23-26_513063_20104/sockets/raylet', 'webui_url': '127.0.0.1:8267', 'session_dir': '/tmp/ray/session_2022-06-24_01-23-26_513063_20104', 'metrics_export_port': 51445, 'gcs_address': '127.0.0.1:52552', 'address': '127.0.0.1:52552', 'node_id': '2c1ab381c3fe5c41a8e4b50b8862e097696fd38410bd0b26a1902e8f'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e46c46",
   "metadata": {},
   "source": [
    "We need to keep track of which URLs we already crawled to avoid double visiting them and we also need to keep track of which URLs still need to be visited. We do this by centralize this data in an actor `CrawlQueue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d56c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import asyncio\n",
    "import collections\n",
    "\n",
    "@ray.remote\n",
    "class CrawlQueue:\n",
    "    # Initialize the crawl queue with a set of seed urls to be crawled.\n",
    "    async def __init__(self, seed_urls):\n",
    "        logging.basicConfig(\n",
    "            format='%(asctime)s %(levelname)s:%(message)s',\n",
    "            level=logging.INFO)\n",
    "        # A queue of pending crawl requests\n",
    "        self.pending_crawl_requests = asyncio.Queue()\n",
    "        # All crawl requests - pending, in progress, and completed\n",
    "        self.all_crawl_requests = set()\n",
    "        for url in seed_urls:\n",
    "            await self.add_crawl_request(url)\n",
    "\n",
    "    # Add additional urls to be crawled - this is called each time a crawler\n",
    "    # encounters a URL in the document it is processing.\n",
    "    async def add_crawl_request(self, url):\n",
    "        if url not in self.all_crawl_requests:\n",
    "            await self.pending_crawl_requests.put(url)\n",
    "            self.all_crawl_requests.add(url)\n",
    "\n",
    "    # Get an url to crawl - this is called from an idle crawler.\n",
    "    # It returns the url to be crawled.\n",
    "    async def get_crawl_request(self):\n",
    "        logging.info(f'URLs: {len(self.all_crawl_requests)}')\n",
    "        return await self.pending_crawl_requests.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6eac9d",
   "metadata": {},
   "source": [
    "<!-- #raw -->\n",
    "```{eval-rst}\n",
    ".. code-block:: python\n",
    "    :emphasize-lines: 21, 30, 39, 40, 42, 45, 46\n",
    "\n",
    "    class RayCrawler:\n",
    "\n",
    "        def __init__(self, crawl_queue):\n",
    "            self.crawl_queue = crawl_queue\n",
    "            self.num_processed_bytes = 0\n",
    "\n",
    "        def download_url(self, url):\n",
    "            text = requests.get(url).text\n",
    "            self.num_processed_bytes += len(text)\n",
    "            return text\n",
    "\n",
    "        def get_linked_urls(self, url, html):\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                path = link.get('href')\n",
    "                if path and path.startswith('/'):\n",
    "                    path = urljoin(url, path)\n",
    "                yield path\n",
    "\n",
    "        def add_url_to_visit(self, url):\n",
    "            self.crawl_queue.add_crawl_request.remote(url)\n",
    "\n",
    "        def crawl(self, url):\n",
    "            html = self.download_url(url)\n",
    "            for url in self.get_linked_urls(url, html):\n",
    "                self.add_url_to_visit(url)\n",
    "\n",
    "        def run(self):\n",
    "            while True:\n",
    "                url = ray.get(self.crawl_queue.get_crawl_request.remote())\n",
    "                logging.info(f'Crawling: {url}')\n",
    "                logging.info(f'Bytes: {self.num_processed_bytes}')\n",
    "                try:\n",
    "                    self.crawl(url)\n",
    "                except Exception:\n",
    "                    # logging.exception(f'Failed to crawl: {url}')\n",
    "                    pass\n",
    "                \n",
    "    @ray.remote\n",
    "    def worker(crawl_queue):\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        RayCrawler(crawl_queue).run()\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        crawl_queue = CrawlQueue.remote(['https://en.wikipedia.org/'])\n",
    "        ray.get([worker.remote(crawl_queue) for i in range(5)])\n",
    "```\n",
    "<!-- #endraw -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40efb21",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:42,862 INFO:URLs: 1\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:42,863 INFO:URLs: 1\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:42,863 INFO:URLs: 1\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:42,863 INFO:URLs: 1\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:43,065 INFO:URLs: 40\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:43,065 INFO:URLs: 40\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:43,065 INFO:URLs: 40\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:43,161 INFO:URLs: 262\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:43,264 INFO:URLs: 327\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:43,637 INFO:URLs: 997\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:43,904 INFO:URLs: 1664\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:44,828 INFO:URLs: 3537\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:44,933 INFO:URLs: 3740\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:45,448 INFO:URLs: 5085\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:46,652 INFO:URLs: 7862\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:46,723 INFO:URLs: 7979\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:46,744 INFO:URLs: 7979\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:47,662 INFO:URLs: 10000\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:47,711 INFO:URLs: 10079\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:48,267 INFO:URLs: 11288\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:49,029 INFO:URLs: 12996\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:49,069 INFO:URLs: 13054\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:49,903 INFO:URLs: 14534\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:50,250 INFO:URLs: 15093\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:50,622 INFO:URLs: 15850\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:50,678 INFO:URLs: 15961\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:50,890 INFO:URLs: 16365\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:51,030 INFO:URLs: 16596\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:51,331 INFO:URLs: 17192\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:51,335 INFO:URLs: 17198\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:51,434 INFO:URLs: 17350\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:51,746 INFO:URLs: 17486\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:51,927 INFO:URLs: 17806\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:51,980 INFO:URLs: 17875\n",
      "2022-06-24 01:23:52,564\tWARNING worker.py:1404 -- Warning: More than 5000 tasks are pending submission to actor 5f0fdb5035e172d9bb00d39b01000000. To reduce memory usage, wait for these tasks to finish before sending more.\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:53,238 INFO:URLs: 20569\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:54,835 INFO:URLs: 25175\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:55,689 INFO:URLs: 26614\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:56,443 INFO:URLs: 27648\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:57,483 INFO:URLs: 29637\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:58,491 INFO:URLs: 31531\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:59,273 INFO:URLs: 32863\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:23:59,978 INFO:URLs: 33858\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:00,229 INFO:URLs: 34293\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:00,699 INFO:URLs: 35119\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:00,791 INFO:URLs: 35281\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:00,987 INFO:URLs: 35740\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:01,034 INFO:URLs: 35796\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:01,102 INFO:URLs: 35916\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:01,404 INFO:URLs: 36113\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:01,848 INFO:URLs: 36783\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:01,922 INFO:URLs: 36918\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:02,332 INFO:URLs: 37714\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:02,799 INFO:URLs: 38609\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:03,169 INFO:URLs: 39630\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:03,547 INFO:URLs: 40015\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:03,630 INFO:URLs: 40130\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:04,569 INFO:URLs: 41238\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:05,444 INFO:URLs: 42255\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:05,493 INFO:URLs: 42299\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:06,600 INFO:URLs: 43529\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:06,712 INFO:URLs: 43615\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:06,984 INFO:URLs: 43969\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:08,501 INFO:URLs: 46122\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:08,605 INFO:URLs: 46244\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:10,775 INFO:URLs: 49757\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:10,867 INFO:URLs: 49930\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:11,866 INFO:URLs: 50991\n",
      "2022-06-24 01:24:12,448\tWARNING worker.py:1404 -- Warning: More than 5000 tasks are pending submission to actor 5f0fdb5035e172d9bb00d39b01000000. To reduce memory usage, wait for these tasks to finish before sending more.\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:12,695 INFO:URLs: 52009\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:13,434 INFO:URLs: 53378\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:15,572 INFO:URLs: 56747\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:18,551 INFO:URLs: 61107\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:19,181 INFO:URLs: 62028\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:19,717 INFO:URLs: 62908\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:19,879 INFO:URLs: 63109\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:20,053 INFO:URLs: 63311\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:20,148 INFO:URLs: 63417\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:20,415 INFO:URLs: 63798\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:20,535 INFO:URLs: 63931\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:21,370 INFO:URLs: 65354\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:21,406 INFO:URLs: 65423\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:21,560 INFO:URLs: 65674\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:22,653 INFO:URLs: 66384\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:23,023 INFO:URLs: 66932\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:23,153 INFO:URLs: 67007\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:24,631 INFO:URLs: 69091\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:24,798 INFO:URLs: 69352\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:27,194 INFO:URLs: 72397\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:27,276 INFO:URLs: 72568\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:28,088 INFO:URLs: 73582\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:28,089 INFO:URLs: 73582\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:28,413 INFO:URLs: 74055\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:29,438 INFO:URLs: 75363\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:29,832 INFO:URLs: 75932\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:30,353 INFO:URLs: 76676\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:31,444 INFO:URLs: 77963\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:31,774 INFO:URLs: 78581\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:32,001 INFO:URLs: 78909\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:32,288 INFO:URLs: 79222\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:32,746 INFO:URLs: 79767\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:33,328 INFO:URLs: 80571\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:34,868 INFO:URLs: 82719\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:35,182 INFO:URLs: 83124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:36,195 INFO:URLs: 84420\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:36,733 INFO:URLs: 85094\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:36,935 INFO:URLs: 85221\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:37,995 INFO:URLs: 86190\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:38,816 INFO:URLs: 86995\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:39,070 INFO:URLs: 87348\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:39,215 INFO:URLs: 87517\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:39,383 INFO:URLs: 87707\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:40,026 INFO:URLs: 88526\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:40,654 INFO:URLs: 89269\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:40,856 INFO:URLs: 89389\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:41,212 INFO:URLs: 89914\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:41,452 INFO:URLs: 90209\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:41,878 INFO:URLs: 90791\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,152 INFO:URLs: 91179\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,213 INFO:URLs: 91263\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,285 INFO:URLs: 91341\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,352 INFO:URLs: 91407\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,419 INFO:URLs: 91462\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,507 INFO:URLs: 91567\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,569 INFO:URLs: 91629\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,654 INFO:URLs: 91672\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,795 INFO:URLs: 91802\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,882 INFO:URLs: 91849\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:42,948 INFO:URLs: 91900\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:43,220 INFO:URLs: 92254\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:43,302 INFO:URLs: 92331\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:43,441 INFO:URLs: 92521\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:43,662 INFO:URLs: 92930\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:43,793 INFO:URLs: 93069\n",
      "\u001b[2m\u001b[36m(CrawlQueue pid=20236)\u001b[0m 2022-06-24 01:24:44,032 INFO:URLs: 93459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e3e2a1a43a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mcrawl_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawlQueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'https://en.wikipedia.org/'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawl_queue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1825\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1826\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         data_metadata_pairs = self.core_worker.get_objects(\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         )\n\u001b[1;32m    367\u001b[0m         \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class RayCrawler:\n",
    "\n",
    "    def __init__(self, crawl_queue):\n",
    "        self.crawl_queue = crawl_queue\n",
    "\n",
    "    def download_url(self, url):\n",
    "        text = requests.get(url).text\n",
    "        return text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def add_url_to_visit(self, url):\n",
    "        self.crawl_queue.add_crawl_request.remote(url)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        for url in self.get_linked_urls(url, html):\n",
    "            self.add_url_to_visit(url)\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            url = ray.get(self.crawl_queue.get_crawl_request.remote())\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "            except Exception:\n",
    "                # logging.exception(f'Failed to crawl: {url}')\n",
    "                pass\n",
    "                \n",
    "@ray.remote\n",
    "def worker(crawl_queue):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    RayCrawler(crawl_queue).run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawl_queue = CrawlQueue.remote(['https://en.wikipedia.org/'])\n",
    "    ray.get([worker.remote(crawl_queue) for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce859e4",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
